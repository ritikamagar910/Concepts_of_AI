{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL7fyZPwLciP",
        "outputId": "af744cde-bf47-4e0b-c479-e21bdcf06067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset has been successfully loaded.\n",
            "\n",
            "Displaying the first 5 rows of the dataset:\n",
            "    Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Displaying the last 5 rows of the dataset:\n",
            "      Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "Detailed information about the dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "\n",
            "Statistical summary of numerical columns in the dataset:\n",
            "               Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n",
            "\n",
            "Feature set (X) containing 'Reading' and 'Writing' scores:\n",
            "    Reading  Writing\n",
            "0       68       63\n",
            "1       81       72\n",
            "2       80       78\n",
            "3       83       79\n",
            "4       64       62\n",
            "\n",
            "Target variable (Y) containing 'Math' scores:\n",
            "    Math\n",
            "0    48\n",
            "1    62\n",
            "2    79\n",
            "3    76\n",
            "4    59\n"
          ]
        }
      ],
      "source": [
        "#Implementation from Scratch\n",
        "#1\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Week5/student.csv\")\n",
        "\n",
        "print(\"The dataset has been successfully loaded.\\n\")\n",
        "\n",
        "print(\"Displaying the first 5 rows of the dataset:\\n\", data.head())\n",
        "print(\"\\nDisplaying the last 5 rows of the dataset:\\n\", data.tail())\n",
        "\n",
        "print(\"\\nDetailed information about the dataset:\")\n",
        "data.info()\n",
        "\n",
        "print(\"\\nStatistical summary of numerical columns in the dataset:\\n\", data.describe())\n",
        "\n",
        "X = data[[\"Reading\", \"Writing\"]]\n",
        "Y = data[[\"Math\"]]\n",
        "print(\"\\nFeature set (X) containing 'Reading' and 'Writing' scores:\\n\", X.head())\n",
        "print(\"\\nTarget variable (Y) containing 'Math' scores:\\n\", Y.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Week5/student.csv\")\n",
        "\n",
        "X = data[[\"Math\"]].to_numpy().T\n",
        "d = X.shape[0]\n",
        "\n",
        "W = np.random.rand(d, 1)\n",
        "\n",
        "Y = np.dot(W.T, X)\n",
        "\n",
        "print(\"Feature Matrix (X) - Transposed 'Math' scores:\")\n",
        "print(X)\n",
        "\n",
        "print(\"\\nWeight Matrix (W) - Randomly initialized weights:\")\n",
        "print(W)\n",
        "\n",
        "print(\"\\nOutput Matrix (Y) - Result of dot product (W.T * X):\")\n",
        "print(Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiKGdg-BMmQK",
        "outputId": "676373dc-7c66-41ef-f9f9-4050fd893b94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Matrix (X) - Transposed 'Math' scores:\n",
            "[[ 48  62  79  76  59  69  70  46  61  86  62  72  56  81  61  49  60  45\n",
            "   71  75  66  57  67  63  68  68  62  56  82  64  71  69  64  76  61  47\n",
            "   50  75  69  42  73  78  65  51  61  69  51  64  90  58 100  73  62  45\n",
            "   47  53  62  49  77  70  60  82 100  72  62  87  56  96  68  66  68  59\n",
            "   52  60  60  46  62  30  83  69  82  64  57  71  37  69  72  73  70  75\n",
            "   54  71  60  81  58  54  49  64  84  84  33  51  68  32  43  73  82  47\n",
            "   74  36  58  68  92  75  43  68  49  69  91  83  65  79  74  81  41  48\n",
            "   31  53  87  94  77  57  64  55  61  78  94  88  77  90  55  83  68  66\n",
            "   56  65  84  90  80  47  67  79  68  62  62  44  74  67  46  71  73  49\n",
            "   42  55  73  75  96  66  56 100  53  68  72  55  59  76  84  68  61  53\n",
            "   73  61  73  63  51  70  69  61  88  90  60 100  68  85  41  70  91  68\n",
            "   63  81  89  66  59  80  87  30  63  43  73  49  65  72  83  46  65 100\n",
            "   40  73  58  87  61  71  72  81  67  69  53  70  56  84  44  63  82  61\n",
            "   68  75  78  32  62  84  65  41  57  54  58  68  53  30  63  36  85  66\n",
            "   45  69  71  70  31  61  73  66  63  64  63  19  64  96  59  59  75  51\n",
            "  100  45  67  34  82  62  59  84  80  65  71  35  46  72  70  80  72  61\n",
            "   39  53  80  73  68  73  57  56  46  48  78  62  89  89  65  49  65  71\n",
            "   70  48  75  83  51  77  51  79  73  72  68  74  58  70  52  70  71  79\n",
            "   71  73  89  54  90  88  72  69  61  46  98  47  71  70  83  51  85  56\n",
            "   98  53  47  86  93  62  66  79  56  41  79  70  89  33  78  77  58  66\n",
            "   72  94  58  55  65  72  46  76  67  71  88  72  66  79  73  49  58  72\n",
            "   78  95  87  95  64  71  80  72  64  46  63  83  61  60  47  82  59  49\n",
            "   80  61  72  64  92  59  69  54  74  78  69  57  55  63  89 100  60  76\n",
            "   73  64  40  79  90  71  47  48  78  55  76  91  39  35  60  89  82  44\n",
            "   89  87  45  66  81  79  71  43  79  81  64  91  62  78  90  46  81  65\n",
            "   58  73  73  58  46  77  42  82  64  87  60  33  60  56  73  64  64  81\n",
            "   60  79  76  83  65  76  78  71  73  67  64  48  82  58  74  71  60  81\n",
            "   47  76  55  80  79  59  62  86  89  53  70  71  56  54  64  44  61  52\n",
            "   56  70  69  74  73  71  82  59  50  38  81  39  68  40  80  59  78  63\n",
            "   71  79  42  67  76  84  74  59  57  55  55  32  95  62  72  51  46  84\n",
            "   68  43  73  79  81  66  93  82  78  71  67  51  89  56  67  62  72  81\n",
            "   67  49  64  81  99  51  78  76  64  90  67  76  43  63  82  72  69  54\n",
            "   54  59  66  77  70  61  75  52  77  73  55  75  45  67  57 100  67  78\n",
            "   56  57  83  89 100  75  60  73  81  91  61  59  65  57  57  76  77  91\n",
            "   68 100  57  78  80  42  66  60  67  40  64  56  61  76  90  80  72  68\n",
            "   40  79  77  83  96 100  72  79  80  67  53  66  91  52  97  63  69  65\n",
            "   65  84  79  73  92  60  84  82  44  90  69  72  69  57  93  69  75  45\n",
            "   46  90  66  57  65  87  39  87  77  57  59  74  60  75  95  74  53  56\n",
            "   91  74  47  95  49  66 100  33  99  39  56  36  58  52  59  67  55  40\n",
            "   32  91  31  54  69  55  73  98  60  26  95  76  53  67  80  76  74  67\n",
            "   81  95  58  80  83  52  69  59  62  70  65  61  42  69  76  83  62  42\n",
            "   64  61  69  77  80  93  63 100  88  76  72  84  86  59  77  53  87  72\n",
            "   67  65  84  77  77  77  45  61  61  62  68  62  49  13  55  85  78  69\n",
            "   69  45  67  68  81  73  53  80  56  82  69  76  44  25  82  55  65  79\n",
            "   95  62  80  93  57  54 100  67  47  69  84  50  79  50  48  56  72  47\n",
            "   68  79  74  37  73  92  75  61  56  52  96  59  67  89  50  51  76  80\n",
            "   68  46 100  44  48  67  88  74  82  77  56  96  67  60  63  77  50  44\n",
            "   64  50  82  91  31  67  90  89  61  64  69  78  58  68  64  85  68  63\n",
            "   68  34  77  61  53  80  62  79  83  65  58  73  94  86  76  58  61  58\n",
            "   64  51  82  56  65  76  48  85  68  85  62  75  38  63  68  62  73  65\n",
            "   77  63  83  55  87  95  48  36  84  91  79  51  76  82  71  52  60  64\n",
            "   68  37  67  51  78  75  60  67  51  78  64  59  85  74  40  94  58  68\n",
            "   83  83  68  74  85  65  68  75  52  74  60  51  64  76  45  55  45  86\n",
            "   85  58  72  59  60  70  79  77  77  66  68  65  70  70  55  81  62  81\n",
            "   71  42  62  57  80  78  71  86  76  45  71  64  69  84  65  55  64  84\n",
            "   52  67  77  64  58  72  73  89  83  66]]\n",
            "\n",
            "Weight Matrix (W) - Randomly initialized weights:\n",
            "[[0.28779604]]\n",
            "\n",
            "Output Matrix (Y) - Result of dot product (W.T * X):\n",
            "[[13.81420993 17.84335449 22.73588718 21.87249906 16.97996637 19.85792678\n",
            "  20.14572282 13.23861785 17.55555845 24.75045946 17.84335449 20.7213149\n",
            "  16.11657825 23.31147926 17.55555845 14.10200597 17.26776241 12.95082181\n",
            "  20.43351886 21.58470302 18.99453865 16.40437429 19.28233469 18.13115053\n",
            "  19.57013074 19.57013074 17.84335449 16.11657825 23.5992753  18.41894657\n",
            "  20.43351886 19.85792678 18.41894657 21.87249906 17.55555845 13.52641389\n",
            "  14.38980201 21.58470302 19.85792678 12.08743369 21.00911094 22.44809114\n",
            "  18.70674261 14.67759805 17.55555845 19.85792678 14.67759805 18.41894657\n",
            "  25.90164362 16.69217033 28.77960402 21.00911094 17.84335449 12.95082181\n",
            "  13.52641389 15.25319013 17.84335449 14.10200597 22.1602951  20.14572282\n",
            "  17.26776241 23.5992753  28.77960402 20.7213149  17.84335449 25.0382555\n",
            "  16.11657825 27.62841986 19.57013074 18.99453865 19.57013074 16.97996637\n",
            "  14.96539409 17.26776241 17.26776241 13.23861785 17.84335449  8.63388121\n",
            "  23.88707134 19.85792678 23.5992753  18.41894657 16.40437429 20.43351886\n",
            "  10.64845349 19.85792678 20.7213149  21.00911094 20.14572282 21.58470302\n",
            "  15.54098617 20.43351886 17.26776241 23.31147926 16.69217033 15.54098617\n",
            "  14.10200597 18.41894657 24.17486738 24.17486738  9.49726933 14.67759805\n",
            "  19.57013074  9.20947329 12.37522973 21.00911094 23.5992753  13.52641389\n",
            "  21.29690698 10.36065745 16.69217033 19.57013074 26.4772357  21.58470302\n",
            "  12.37522973 19.57013074 14.10200597 19.85792678 26.18943966 23.88707134\n",
            "  18.70674261 22.73588718 21.29690698 23.31147926 11.79963765 13.81420993\n",
            "   8.92167725 15.25319013 25.0382555  27.05282778 22.1602951  16.40437429\n",
            "  18.41894657 15.82878221 17.55555845 22.44809114 27.05282778 25.32605154\n",
            "  22.1602951  25.90164362 15.82878221 23.88707134 19.57013074 18.99453865\n",
            "  16.11657825 18.70674261 24.17486738 25.90164362 23.02368322 13.52641389\n",
            "  19.28233469 22.73588718 19.57013074 17.84335449 17.84335449 12.66302577\n",
            "  21.29690698 19.28233469 13.23861785 20.43351886 21.00911094 14.10200597\n",
            "  12.08743369 15.82878221 21.00911094 21.58470302 27.62841986 18.99453865\n",
            "  16.11657825 28.77960402 15.25319013 19.57013074 20.7213149  15.82878221\n",
            "  16.97996637 21.87249906 24.17486738 19.57013074 17.55555845 15.25319013\n",
            "  21.00911094 17.55555845 21.00911094 18.13115053 14.67759805 20.14572282\n",
            "  19.85792678 17.55555845 25.32605154 25.90164362 17.26776241 28.77960402\n",
            "  19.57013074 24.46266342 11.79963765 20.14572282 26.18943966 19.57013074\n",
            "  18.13115053 23.31147926 25.61384758 18.99453865 16.97996637 23.02368322\n",
            "  25.0382555   8.63388121 18.13115053 12.37522973 21.00911094 14.10200597\n",
            "  18.70674261 20.7213149  23.88707134 13.23861785 18.70674261 28.77960402\n",
            "  11.51184161 21.00911094 16.69217033 25.0382555  17.55555845 20.43351886\n",
            "  20.7213149  23.31147926 19.28233469 19.85792678 15.25319013 20.14572282\n",
            "  16.11657825 24.17486738 12.66302577 18.13115053 23.5992753  17.55555845\n",
            "  19.57013074 21.58470302 22.44809114  9.20947329 17.84335449 24.17486738\n",
            "  18.70674261 11.79963765 16.40437429 15.54098617 16.69217033 19.57013074\n",
            "  15.25319013  8.63388121 18.13115053 10.36065745 24.46266342 18.99453865\n",
            "  12.95082181 19.85792678 20.43351886 20.14572282  8.92167725 17.55555845\n",
            "  21.00911094 18.99453865 18.13115053 18.41894657 18.13115053  5.46812476\n",
            "  18.41894657 27.62841986 16.97996637 16.97996637 21.58470302 14.67759805\n",
            "  28.77960402 12.95082181 19.28233469  9.78506537 23.5992753  17.84335449\n",
            "  16.97996637 24.17486738 23.02368322 18.70674261 20.43351886 10.07286141\n",
            "  13.23861785 20.7213149  20.14572282 23.02368322 20.7213149  17.55555845\n",
            "  11.22404557 15.25319013 23.02368322 21.00911094 19.57013074 21.00911094\n",
            "  16.40437429 16.11657825 13.23861785 13.81420993 22.44809114 17.84335449\n",
            "  25.61384758 25.61384758 18.70674261 14.10200597 18.70674261 20.43351886\n",
            "  20.14572282 13.81420993 21.58470302 23.88707134 14.67759805 22.1602951\n",
            "  14.67759805 22.73588718 21.00911094 20.7213149  19.57013074 21.29690698\n",
            "  16.69217033 20.14572282 14.96539409 20.14572282 20.43351886 22.73588718\n",
            "  20.43351886 21.00911094 25.61384758 15.54098617 25.90164362 25.32605154\n",
            "  20.7213149  19.85792678 17.55555845 13.23861785 28.20401194 13.52641389\n",
            "  20.43351886 20.14572282 23.88707134 14.67759805 24.46266342 16.11657825\n",
            "  28.20401194 15.25319013 13.52641389 24.75045946 26.76503174 17.84335449\n",
            "  18.99453865 22.73588718 16.11657825 11.79963765 22.73588718 20.14572282\n",
            "  25.61384758  9.49726933 22.44809114 22.1602951  16.69217033 18.99453865\n",
            "  20.7213149  27.05282778 16.69217033 15.82878221 18.70674261 20.7213149\n",
            "  13.23861785 21.87249906 19.28233469 20.43351886 25.32605154 20.7213149\n",
            "  18.99453865 22.73588718 21.00911094 14.10200597 16.69217033 20.7213149\n",
            "  22.44809114 27.34062382 25.0382555  27.34062382 18.41894657 20.43351886\n",
            "  23.02368322 20.7213149  18.41894657 13.23861785 18.13115053 23.88707134\n",
            "  17.55555845 17.26776241 13.52641389 23.5992753  16.97996637 14.10200597\n",
            "  23.02368322 17.55555845 20.7213149  18.41894657 26.4772357  16.97996637\n",
            "  19.85792678 15.54098617 21.29690698 22.44809114 19.85792678 16.40437429\n",
            "  15.82878221 18.13115053 25.61384758 28.77960402 17.26776241 21.87249906\n",
            "  21.00911094 18.41894657 11.51184161 22.73588718 25.90164362 20.43351886\n",
            "  13.52641389 13.81420993 22.44809114 15.82878221 21.87249906 26.18943966\n",
            "  11.22404557 10.07286141 17.26776241 25.61384758 23.5992753  12.66302577\n",
            "  25.61384758 25.0382555  12.95082181 18.99453865 23.31147926 22.73588718\n",
            "  20.43351886 12.37522973 22.73588718 23.31147926 18.41894657 26.18943966\n",
            "  17.84335449 22.44809114 25.90164362 13.23861785 23.31147926 18.70674261\n",
            "  16.69217033 21.00911094 21.00911094 16.69217033 13.23861785 22.1602951\n",
            "  12.08743369 23.5992753  18.41894657 25.0382555  17.26776241  9.49726933\n",
            "  17.26776241 16.11657825 21.00911094 18.41894657 18.41894657 23.31147926\n",
            "  17.26776241 22.73588718 21.87249906 23.88707134 18.70674261 21.87249906\n",
            "  22.44809114 20.43351886 21.00911094 19.28233469 18.41894657 13.81420993\n",
            "  23.5992753  16.69217033 21.29690698 20.43351886 17.26776241 23.31147926\n",
            "  13.52641389 21.87249906 15.82878221 23.02368322 22.73588718 16.97996637\n",
            "  17.84335449 24.75045946 25.61384758 15.25319013 20.14572282 20.43351886\n",
            "  16.11657825 15.54098617 18.41894657 12.66302577 17.55555845 14.96539409\n",
            "  16.11657825 20.14572282 19.85792678 21.29690698 21.00911094 20.43351886\n",
            "  23.5992753  16.97996637 14.38980201 10.93624953 23.31147926 11.22404557\n",
            "  19.57013074 11.51184161 23.02368322 16.97996637 22.44809114 18.13115053\n",
            "  20.43351886 22.73588718 12.08743369 19.28233469 21.87249906 24.17486738\n",
            "  21.29690698 16.97996637 16.40437429 15.82878221 15.82878221  9.20947329\n",
            "  27.34062382 17.84335449 20.7213149  14.67759805 13.23861785 24.17486738\n",
            "  19.57013074 12.37522973 21.00911094 22.73588718 23.31147926 18.99453865\n",
            "  26.76503174 23.5992753  22.44809114 20.43351886 19.28233469 14.67759805\n",
            "  25.61384758 16.11657825 19.28233469 17.84335449 20.7213149  23.31147926\n",
            "  19.28233469 14.10200597 18.41894657 23.31147926 28.49180798 14.67759805\n",
            "  22.44809114 21.87249906 18.41894657 25.90164362 19.28233469 21.87249906\n",
            "  12.37522973 18.13115053 23.5992753  20.7213149  19.85792678 15.54098617\n",
            "  15.54098617 16.97996637 18.99453865 22.1602951  20.14572282 17.55555845\n",
            "  21.58470302 14.96539409 22.1602951  21.00911094 15.82878221 21.58470302\n",
            "  12.95082181 19.28233469 16.40437429 28.77960402 19.28233469 22.44809114\n",
            "  16.11657825 16.40437429 23.88707134 25.61384758 28.77960402 21.58470302\n",
            "  17.26776241 21.00911094 23.31147926 26.18943966 17.55555845 16.97996637\n",
            "  18.70674261 16.40437429 16.40437429 21.87249906 22.1602951  26.18943966\n",
            "  19.57013074 28.77960402 16.40437429 22.44809114 23.02368322 12.08743369\n",
            "  18.99453865 17.26776241 19.28233469 11.51184161 18.41894657 16.11657825\n",
            "  17.55555845 21.87249906 25.90164362 23.02368322 20.7213149  19.57013074\n",
            "  11.51184161 22.73588718 22.1602951  23.88707134 27.62841986 28.77960402\n",
            "  20.7213149  22.73588718 23.02368322 19.28233469 15.25319013 18.99453865\n",
            "  26.18943966 14.96539409 27.9162159  18.13115053 19.85792678 18.70674261\n",
            "  18.70674261 24.17486738 22.73588718 21.00911094 26.4772357  17.26776241\n",
            "  24.17486738 23.5992753  12.66302577 25.90164362 19.85792678 20.7213149\n",
            "  19.85792678 16.40437429 26.76503174 19.85792678 21.58470302 12.95082181\n",
            "  13.23861785 25.90164362 18.99453865 16.40437429 18.70674261 25.0382555\n",
            "  11.22404557 25.0382555  22.1602951  16.40437429 16.97996637 21.29690698\n",
            "  17.26776241 21.58470302 27.34062382 21.29690698 15.25319013 16.11657825\n",
            "  26.18943966 21.29690698 13.52641389 27.34062382 14.10200597 18.99453865\n",
            "  28.77960402  9.49726933 28.49180798 11.22404557 16.11657825 10.36065745\n",
            "  16.69217033 14.96539409 16.97996637 19.28233469 15.82878221 11.51184161\n",
            "   9.20947329 26.18943966  8.92167725 15.54098617 19.85792678 15.82878221\n",
            "  21.00911094 28.20401194 17.26776241  7.48269705 27.34062382 21.87249906\n",
            "  15.25319013 19.28233469 23.02368322 21.87249906 21.29690698 19.28233469\n",
            "  23.31147926 27.34062382 16.69217033 23.02368322 23.88707134 14.96539409\n",
            "  19.85792678 16.97996637 17.84335449 20.14572282 18.70674261 17.55555845\n",
            "  12.08743369 19.85792678 21.87249906 23.88707134 17.84335449 12.08743369\n",
            "  18.41894657 17.55555845 19.85792678 22.1602951  23.02368322 26.76503174\n",
            "  18.13115053 28.77960402 25.32605154 21.87249906 20.7213149  24.17486738\n",
            "  24.75045946 16.97996637 22.1602951  15.25319013 25.0382555  20.7213149\n",
            "  19.28233469 18.70674261 24.17486738 22.1602951  22.1602951  22.1602951\n",
            "  12.95082181 17.55555845 17.55555845 17.84335449 19.57013074 17.84335449\n",
            "  14.10200597  3.74134852 15.82878221 24.46266342 22.44809114 19.85792678\n",
            "  19.85792678 12.95082181 19.28233469 19.57013074 23.31147926 21.00911094\n",
            "  15.25319013 23.02368322 16.11657825 23.5992753  19.85792678 21.87249906\n",
            "  12.66302577  7.19490101 23.5992753  15.82878221 18.70674261 22.73588718\n",
            "  27.34062382 17.84335449 23.02368322 26.76503174 16.40437429 15.54098617\n",
            "  28.77960402 19.28233469 13.52641389 19.85792678 24.17486738 14.38980201\n",
            "  22.73588718 14.38980201 13.81420993 16.11657825 20.7213149  13.52641389\n",
            "  19.57013074 22.73588718 21.29690698 10.64845349 21.00911094 26.4772357\n",
            "  21.58470302 17.55555845 16.11657825 14.96539409 27.62841986 16.97996637\n",
            "  19.28233469 25.61384758 14.38980201 14.67759805 21.87249906 23.02368322\n",
            "  19.57013074 13.23861785 28.77960402 12.66302577 13.81420993 19.28233469\n",
            "  25.32605154 21.29690698 23.5992753  22.1602951  16.11657825 27.62841986\n",
            "  19.28233469 17.26776241 18.13115053 22.1602951  14.38980201 12.66302577\n",
            "  18.41894657 14.38980201 23.5992753  26.18943966  8.92167725 19.28233469\n",
            "  25.90164362 25.61384758 17.55555845 18.41894657 19.85792678 22.44809114\n",
            "  16.69217033 19.57013074 18.41894657 24.46266342 19.57013074 18.13115053\n",
            "  19.57013074  9.78506537 22.1602951  17.55555845 15.25319013 23.02368322\n",
            "  17.84335449 22.73588718 23.88707134 18.70674261 16.69217033 21.00911094\n",
            "  27.05282778 24.75045946 21.87249906 16.69217033 17.55555845 16.69217033\n",
            "  18.41894657 14.67759805 23.5992753  16.11657825 18.70674261 21.87249906\n",
            "  13.81420993 24.46266342 19.57013074 24.46266342 17.84335449 21.58470302\n",
            "  10.93624953 18.13115053 19.57013074 17.84335449 21.00911094 18.70674261\n",
            "  22.1602951  18.13115053 23.88707134 15.82878221 25.0382555  27.34062382\n",
            "  13.81420993 10.36065745 24.17486738 26.18943966 22.73588718 14.67759805\n",
            "  21.87249906 23.5992753  20.43351886 14.96539409 17.26776241 18.41894657\n",
            "  19.57013074 10.64845349 19.28233469 14.67759805 22.44809114 21.58470302\n",
            "  17.26776241 19.28233469 14.67759805 22.44809114 18.41894657 16.97996637\n",
            "  24.46266342 21.29690698 11.51184161 27.05282778 16.69217033 19.57013074\n",
            "  23.88707134 23.88707134 19.57013074 21.29690698 24.46266342 18.70674261\n",
            "  19.57013074 21.58470302 14.96539409 21.29690698 17.26776241 14.67759805\n",
            "  18.41894657 21.87249906 12.95082181 15.82878221 12.95082181 24.75045946\n",
            "  24.46266342 16.69217033 20.7213149  16.97996637 17.26776241 20.14572282\n",
            "  22.73588718 22.1602951  22.1602951  18.99453865 19.57013074 18.70674261\n",
            "  20.14572282 20.14572282 15.82878221 23.31147926 17.84335449 23.31147926\n",
            "  20.43351886 12.08743369 17.84335449 16.40437429 23.02368322 22.44809114\n",
            "  20.43351886 24.75045946 21.87249906 12.95082181 20.43351886 18.41894657\n",
            "  19.85792678 24.17486738 18.70674261 15.82878221 18.41894657 24.17486738\n",
            "  14.96539409 19.28233469 22.1602951  18.41894657 16.69217033 20.7213149\n",
            "  21.00911094 25.61384758 23.88707134 18.99453865]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Week5/student.csv\")\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(data) * train_ratio)\n",
        "\n",
        "train_data = data.iloc[:train_size]\n",
        "test_data = data.iloc[train_size:]\n",
        "\n",
        "X_train = train_data.drop('Math', axis=1)\n",
        "y_train = train_data['Math']\n",
        "\n",
        "X_test = test_data.drop('Math', axis=1)\n",
        "y_test = test_data['Math']\n",
        "\n",
        "print(f\"Training data size (X_train): {len(X_train)} rows\")\n",
        "print(f\"Test data size (X_test): {len(X_test)} rows\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj9dSad9M8Wy",
        "outputId": "b4345f4c-8392-4e86-87a2-ec71d6e8c5dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size (X_train): 800 rows\n",
            "Test data size (X_test): 200 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def gradient_descent(X, y, W, learning_rate, num_iterations):\n",
        "    m = len(y)\n",
        "    for i in range(num_iterations):\n",
        "        predictions = X.dot(W)\n",
        "        errors = predictions - y\n",
        "        gradient = (1/m) * X.T.dot(errors)\n",
        "        W -= learning_rate * gradient\n",
        "        if (i+1) % 100 == 0:\n",
        "            loss = np.mean(errors**2)\n",
        "            print(f\"Iteration {i+1}: Loss = {loss}\")\n",
        "    return W\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "\n",
        "def r2_score_custom(y_true, y_pred):\n",
        "    total_variance = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    unexplained_variance = np.sum((y_true - y_pred) ** 2)\n",
        "    return 1 - (unexplained_variance / total_variance)\n",
        "\n",
        "def train_and_evaluate_model(data, target_column, learning_rate=0.01, num_iterations=1000, train_ratio=0.8):\n",
        "    X = data.drop(target_column, axis=1).values\n",
        "    y = data[target_column].values.reshape(-1, 1)\n",
        "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "    train_size = int(len(data) * train_ratio)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "    W = np.zeros((X_train.shape[1], 1))\n",
        "    print(\"Starting Gradient Descent...\")\n",
        "    W = gradient_descent(X_train, y_train, W, learning_rate, num_iterations)\n",
        "    print(\"Gradient Descent Completed.\\n\")\n",
        "    y_pred = X_test.dot(W)\n",
        "    rmse_value = rmse(y_test, y_pred)\n",
        "    r2_value = r2_score_custom(y_test, y_pred)\n",
        "    print(f\"Model Evaluation Results:\")\n",
        "    print(f\"RMSE (Root Mean Squared Error): {rmse_value:.4f}\")\n",
        "    print(f\"R² (Coefficient of Determination): {r2_value:.4f}\\n\")\n",
        "    return W\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Week5/student.csv\")\n",
        "W = train_and_evaluate_model(data, target_column='Math', learning_rate=0.01, num_iterations=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF9hUzfON9Rh",
        "outputId": "4d360a2b-449a-43b8-8036-abf47f103e78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 699.2962963635672\n",
            "Iteration 200: Loss = 162.49871532444774\n",
            "Iteration 300: Loss = 90.90732975530241\n",
            "Iteration 400: Loss = 81.3166534173156\n",
            "Iteration 500: Loss = 80.02741601616349\n",
            "Iteration 600: Loss = 79.85066715182437\n",
            "Iteration 700: Loss = 79.82332612061171\n",
            "Iteration 800: Loss = 79.81633321119472\n",
            "Iteration 900: Loss = 79.81236775342418\n",
            "Iteration 1000: Loss = 79.80908195225282\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "RMSE (Root Mean Squared Error): 8.0630\n",
            "R² (Coefficient of Determination): 0.6688\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def gradient_descent(X, y, W, learning_rate, num_iterations):\n",
        "    m = len(y)\n",
        "    for i in range(num_iterations):\n",
        "        predictions = X.dot(W)\n",
        "        errors = predictions - y\n",
        "        gradient = (1/m) * X.T.dot(errors)\n",
        "        W -= learning_rate * gradient\n",
        "        if (i+1) % 100 == 0:\n",
        "            loss = np.mean(errors**2)\n",
        "            print(f\"Iteration {i+1}: Loss = {loss}\")\n",
        "    return W\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "\n",
        "def r2_score_custom(y_true, y_pred):\n",
        "    total_variance = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    unexplained_variance = np.sum((y_true - y_pred) ** 2)\n",
        "    return 1 - (unexplained_variance / total_variance)\n",
        "\n",
        "def train_and_evaluate_model(data, target_column, learning_rate=0.01, num_iterations=1000, train_ratio=0.8):\n",
        "    X = data.drop(target_column, axis=1).values\n",
        "    y = data[target_column].values.reshape(-1, 1)\n",
        "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "    train_size = int(len(data) * train_ratio)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "    W = np.zeros((X_train.shape[1], 1))\n",
        "    print(\"Starting Gradient Descent...\")\n",
        "    W = gradient_descent(X_train, y_train, W, learning_rate, num_iterations)\n",
        "    print(\"Gradient Descent Completed.\\n\")\n",
        "    y_pred = X_test.dot(W)\n",
        "    rmse_value = rmse(y_test, y_pred)\n",
        "    r2_value = r2_score_custom(y_test, y_pred)\n",
        "    print(f\"Model Evaluation Results:\")\n",
        "    print(f\"Learning Rate: {learning_rate}\")\n",
        "    print(f\"RMSE (Root Mean Squared Error): {rmse_value:.4f}\")\n",
        "    print(f\"R² (Coefficient of Determination): {r2_value:.4f}\\n\")\n",
        "    return W\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Week5/student.csv\")\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5, 1]\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Evaluating model with learning rate: {lr}\")\n",
        "    W = train_and_evaluate_model(data, target_column='Math', learning_rate=lr, num_iterations=1000)\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHtJiwhfOJ-l",
        "outputId": "c527e6e4-99e8-44dc-9b96-2bf0b5dec574"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model with learning rate: 0.001\n",
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 3889.0670099762856\n",
            "Iteration 200: Loss = 3181.745091095462\n",
            "Iteration 300: Loss = 2608.1846780395454\n",
            "Iteration 400: Loss = 2142.3042431056147\n",
            "Iteration 500: Loss = 1763.3582670188446\n",
            "Iteration 600: Loss = 1454.7681111661116\n",
            "Iteration 700: Loss = 1203.2318619387745\n",
            "Iteration 800: Loss = 998.0403036668321\n",
            "Iteration 900: Loss = 830.546782573105\n",
            "Iteration 1000: Loss = 693.7531351203352\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "Learning Rate: 0.001\n",
            "RMSE (Root Mean Squared Error): 26.4877\n",
            "R² (Coefficient of Determination): -2.5742\n",
            "\n",
            "--------------------------------------------------\n",
            "Evaluating model with learning rate: 0.01\n",
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 699.2962963635672\n",
            "Iteration 200: Loss = 162.49871532444774\n",
            "Iteration 300: Loss = 90.90732975530241\n",
            "Iteration 400: Loss = 81.3166534173156\n",
            "Iteration 500: Loss = 80.02741601616349\n",
            "Iteration 600: Loss = 79.85066715182437\n",
            "Iteration 700: Loss = 79.82332612061171\n",
            "Iteration 800: Loss = 79.81633321119472\n",
            "Iteration 900: Loss = 79.81236775342418\n",
            "Iteration 1000: Loss = 79.80908195225282\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "Learning Rate: 0.01\n",
            "RMSE (Root Mean Squared Error): 8.0630\n",
            "R² (Coefficient of Determination): 0.6688\n",
            "\n",
            "--------------------------------------------------\n",
            "Evaluating model with learning rate: 0.1\n",
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 79.80928990194519\n",
            "Iteration 200: Loss = 79.78923742259309\n",
            "Iteration 300: Loss = 79.7814878345864\n",
            "Iteration 400: Loss = 79.77849230088077\n",
            "Iteration 500: Loss = 79.7773344042063\n",
            "Iteration 600: Loss = 79.7768868296368\n",
            "Iteration 700: Loss = 79.7767138237031\n",
            "Iteration 800: Loss = 79.77664694981183\n",
            "Iteration 900: Loss = 79.77662110030924\n",
            "Iteration 1000: Loss = 79.77661110841598\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "Learning Rate: 0.1\n",
            "RMSE (Root Mean Squared Error): 8.0551\n",
            "R² (Coefficient of Determination): 0.6694\n",
            "\n",
            "--------------------------------------------------\n",
            "Evaluating model with learning rate: 0.5\n",
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 79.77732906156535\n",
            "Iteration 200: Loss = 79.77661078235637\n",
            "Iteration 300: Loss = 79.77660486172364\n",
            "Iteration 400: Loss = 79.77660481292104\n",
            "Iteration 500: Loss = 79.77660481251877\n",
            "Iteration 600: Loss = 79.77660481251546\n",
            "Iteration 700: Loss = 79.77660481251543\n",
            "Iteration 800: Loss = 79.77660481251542\n",
            "Iteration 900: Loss = 79.77660481251543\n",
            "Iteration 1000: Loss = 79.77660481251542\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "Learning Rate: 0.5\n",
            "RMSE (Root Mean Squared Error): 8.0551\n",
            "R² (Coefficient of Determination): 0.6695\n",
            "\n",
            "--------------------------------------------------\n",
            "Evaluating model with learning rate: 1\n",
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 5724.303316873511\n",
            "Iteration 200: Loss = 202613.34169789232\n",
            "Iteration 300: Loss = 7267269.236823377\n",
            "Iteration 400: Loss = 260757069.00177982\n",
            "Iteration 500: Loss = 9356327969.944635\n",
            "Iteration 600: Loss = 335718217445.5252\n",
            "Iteration 700: Loss = 12046042292983.225\n",
            "Iteration 800: Loss = 432228956953977.25\n",
            "Iteration 900: Loss = 1.55089835056876e+16\n",
            "Iteration 1000: Loss = 5.564841631037286e+17\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "Learning Rate: 1\n",
            "RMSE (Root Mean Squared Error): 694744374.0630\n",
            "R² (Coefficient of Determination): -2458907675230898.0000\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1GSMPnFLgXb",
        "outputId": "69593d86-6233-4601-8746-68e6557d2041"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}